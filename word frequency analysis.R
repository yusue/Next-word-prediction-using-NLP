install.packages("tidyverse")
install.packages("tokenizers")
library(tidyverse)
library(tokenizers)
twitter = file("en_US.twitter.txt","r")
text_twitter=paste(readLines(twitter),collapse = "\n")
words_twitter=tokenize_words(text_twitter)
word_count=length(words_twitter[[1]])
twitter_word_freq = table(words_twitter[[1]])
df_twitter=data_frame(word=names(twitter_word_freq),count=as.numeric(twitter_word_freq))
Line_twitter=readLines(twitter)
twitter_word =tokenize_words(Line_twitter)
twitter_word_count=length(twitter_word[[1]])
twitter_word_freq = table(unlist(twitter_word))
df=cbind(names(twitter_word_freq),as.integer(twitter_word_freq))
line_tw=length(readLines(twitter))

